{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c1a1f1-bb3d-44d1-9e0d-fb34696dcd5c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c195396-0a06-4cef-9275-3e8cac5560b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import library untuk data preparation dan visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import pickle and json file for columns and model file\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import yaml\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba3c7c7-71e1-46b0-826d-814c4965c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data yang sudah dibuat sebelumnya dari proses data preparation\n",
    "X_train = joblib.load(\"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_train.pkl\")\n",
    "y_train = joblib.load(\"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\y_train.pkl\")\n",
    "#Import data yang sudah dibuat sebelumnya dari proses data preparation\n",
    "X_test = joblib.load(\"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_test.pkl\")\n",
    "y_test = joblib.load(\"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\y_test.pkl\")\n",
    "#Import data yang sudah dibuat sebelumnya dari proses data preparation\n",
    "X_valid = joblib.load(\"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_valid.pkl\")\n",
    "y_valid = joblib.load(\"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\y_valid.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f784e85-4e16-434b-ba00-39e3cfeec410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>Imax 84532 Tenor Large Recycled Glass Vase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120</th>\n",
       "      <td>Darice 6202-113 10-Light 5-Point Star Tree Top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>5 Piece Full Size Frozen Bedding Set Includes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>New Star Foodservice Knives (Set of 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13665</th>\n",
       "      <td>Mr &amp;amp; Mrs T Bold &amp;amp; Spicy Bloody Mary Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19690</th>\n",
       "      <td>Aviditi MD24126 Multi-Depth Corrugated Box, 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15580</th>\n",
       "      <td>Perfume Studio&amp;reg; Graduated Glass Dropper Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10534</th>\n",
       "      <td>Brother HL-L2340DW Compact Laser Printer, Mono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>Hello Kitty Wastebasket - Garbage Can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7250</th>\n",
       "      <td>Newport Storage Platform Bed Finish: Tan, Size...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8052 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title\n",
       "5544          Imax 84532 Tenor Large Recycled Glass Vase\n",
       "3120   Darice 6202-113 10-Light 5-Point Star Tree Top...\n",
       "1387   5 Piece Full Size Frozen Bedding Set Includes ...\n",
       "1075             New Star Foodservice Knives (Set of 12)\n",
       "13665  Mr &amp; Mrs T Bold &amp; Spicy Bloody Mary Mi...\n",
       "...                                                  ...\n",
       "19690  Aviditi MD24126 Multi-Depth Corrugated Box, 24...\n",
       "15580  Perfume Studio&reg; Graduated Glass Dropper Bo...\n",
       "10534  Brother HL-L2340DW Compact Laser Printer, Mono...\n",
       "2793               Hello Kitty Wastebasket - Garbage Can\n",
       "7250   Newport Storage Platform Bed Finish: Tan, Size...\n",
       "\n",
       "[8052 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f81a3f27-3540-42c2-b407-88fae165b016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544              Home & Kitchen\n",
       "3120              Home & Kitchen\n",
       "1387              Home & Kitchen\n",
       "1075              Home & Kitchen\n",
       "13665     Grocery & Gourmet Food\n",
       "                  ...           \n",
       "19690            Office Products\n",
       "15580    Industrial & Scientific\n",
       "10534            Office Products\n",
       "2793              Home & Kitchen\n",
       "7250              Home & Kitchen\n",
       "Name: category, Length: 8052, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ecb653-932c-47d6-b67e-dd49905edef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stopwords from the NLTK library\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(df, column_name):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x).lower())\n",
    "    \n",
    "    # Remove stopwords and join the words with a single space\n",
    "    df[column_name] = df[column_name].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2500d4-1ecb-4f12-858d-3115b6860127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan teks dari karakter khusus dan mengonversi teks menjadi huruf kecil\n",
    "X_train = preprocess_text(X_train, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b6901d-750a-4ec1-b864-8a82b3655565",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess_text(X_test, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbd1284-d13d-4fa4-a200-c9e8b6650e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = preprocess_text(X_valid, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0dec50b-eeea-483c-9b69-51c0ee50e0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_vectorizer_model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "\n",
    "# Inisialisasi CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fitting hanya pada data pelatihan\n",
    "count_vectorizer.fit(X_train['title'])\n",
    "\n",
    "# Transformasi pada data pelatihan, data uji, dan data validasi\n",
    "X_train_bow = count_vectorizer.transform(X_train['title'])\n",
    "X_test_bow = count_vectorizer.transform(X_test['title'])\n",
    "X_valid_bow = count_vectorizer.transform(X_valid['title'])\n",
    "\n",
    "# Menyimpan model CountVectorizer ke dalam file\n",
    "joblib.dump(count_vectorizer, 'count_vectorizer_model.pkl')\n",
    "\n",
    "# Kemudian, di kemudian hari, Anda dapat memuat model CountVectorizer dan menggunakannya pada data baru\n",
    "# untuk melakukan transformasi seperti ini:\n",
    "\n",
    "# Memuat model CountVectorizer\n",
    "# loaded_count_vectorizer = joblib.load('count_vectorizer_model.pkl')\n",
    "\n",
    "# Menggunakan model CountVectorizer untuk melakukan transformasi pada data baru\n",
    "# new_data = [\"Contoh teks baru untuk di-encode\"]\n",
    "# X_new_bow = loaded_count_vectorizer.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f585be-1e3c-47c1-ba36-6890fb43cbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer_model.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# Inisialisasi TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fitting hanya pada data pelatihan\n",
    "tfidf_vectorizer.fit(X_train['title'])\n",
    "\n",
    "# Transformasi pada data pelatihan, data uji, dan data validasi\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train['title'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid['title'])\n",
    "\n",
    "# Menyimpan model TfidfVectorizer ke dalam file\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer_model.pkl')\n",
    "\n",
    "# Kemudian, di kemudian hari, Anda dapat memuat model TfidfVectorizer dan menggunakannya pada data baru\n",
    "# untuk melakukan transformasi seperti ini:\n",
    "# Memuat model TfidfVectorizer\n",
    "# loaded_tfidf_vectorizer = joblib.load('tfidf_vectorizer_model.pkl')\n",
    "# X_new_tfidf = loaded_tfidf_vectorizer.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae49c1f-4625-4871-9dc4-88fe70afe260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\n\\n# One-Hot Encoding (BoW) pada kolom 'title'\\ncount_vectorizer = CountVectorizer()\\nX_train_bow = count_vectorizer.fit_transform(X_train['title'])\\nX_test_bow = count_vectorizer.transform(X_test['title'])\\nX_valid_bow = count_vectorizer.transform(X_valid['title'])\\n\\n# TF-IDF pada kolom 'title'\\ntfidf_vectorizer = TfidfVectorizer()\\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train['title'])\\nX_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\\nX_valid_tfidf = tfidf_vectorizer.transform(X_valid['title'])\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# One-Hot Encoding (BoW) pada kolom 'title'\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train['title'])\n",
    "X_test_bow = count_vectorizer.transform(X_test['title'])\n",
    "X_valid_bow = count_vectorizer.transform(X_valid['title'])\n",
    "\n",
    "# TF-IDF pada kolom 'title'\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['title'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid['title'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533acafd-9a6d-443c-a707-34a2c2f3667c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8052x20797 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 79263 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe5feec-c3cf-41ff-b965-4310de4e4feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8052x20797 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 79263 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "975c4259-f6b9-405c-aa4d-58e7aa52eaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544              Home & Kitchen\n",
       "3120              Home & Kitchen\n",
       "1387              Home & Kitchen\n",
       "1075              Home & Kitchen\n",
       "13665     Grocery & Gourmet Food\n",
       "                  ...           \n",
       "19690            Office Products\n",
       "15580    Industrial & Scientific\n",
       "10534            Office Products\n",
       "2793              Home & Kitchen\n",
       "7250              Home & Kitchen\n",
       "Name: category, Length: 8052, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cae5763-b9b1-4dab-87e3-26fe43f0ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "y_valid_encoded = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4948f63d-ecc2-4f83-86ce-1898f3ffa22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\y_test_encoded.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_train_bow, \"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_train_bow.pkl\")\n",
    "joblib.dump(y_train_encoded, \"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\y_train_encoded.pkl\")\n",
    "joblib.dump(X_valid_bow, \"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_valid_bow.pkl\")\n",
    "joblib.dump(y_valid_encoded, \"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\y_valid_encoded.pkl\")\n",
    "joblib.dump(X_test_bow, \"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_test_bow.pkl\")\n",
    "joblib.dump(y_test_encoded, \"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\y_test_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da5ec7e-ec09-4fb5-ac11-1320b250497b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_test_tfidf.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_train_tfidf, \"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_train_tfidf.pkl\")\n",
    "joblib.dump(X_valid_tfidf, \"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_valid_tfidf.pkl\")\n",
    "joblib.dump(X_test_tfidf, \"C:\\\\Users\\\\hp\\\\Portofolio Data Science\\\\6 - E-Commerce Product Classification\\\\dataset\\\\2 - processed\\\\X_test_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311234bc-8d54-4a66-b5ae-d6a5797fac44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a71bbe-1443-4223-be11-6b96c54e0429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be1ab4-5df1-401d-b288-750169780b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd35381-2667-4ea9-8f1b-a2a9bdc83262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8f70028-54da-4182-a400-18f3b493c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh nilai yang telah di-decode untuk data uji:\n",
      "['Tools & Home Improvement' 'Grocery & Gourmet Food' 'Home & Kitchen'\n",
      " 'Home & Kitchen' 'Tools & Home Improvement' 'Office Products'\n",
      " 'Home & Kitchen' 'Home & Kitchen' 'Tools & Home Improvement'\n",
      " 'Home & Kitchen']\n",
      "Contoh nilai yang telah di-decode untuk data validasi:\n",
      "['Tools & Home Improvement' 'Grocery & Gourmet Food' 'Home & Kitchen'\n",
      " 'Home & Kitchen' 'Home & Kitchen' 'Tools & Home Improvement'\n",
      " 'Tools & Home Improvement' 'Home & Kitchen' 'Tools & Home Improvement'\n",
      " 'Home & Kitchen']\n"
     ]
    }
   ],
   "source": [
    "# Mendekode nilai yang telah diencode untuk data uji\n",
    "y_test_decoded = le.inverse_transform(y_test_encoded)\n",
    "\n",
    "# Mendekode nilai yang telah diencode untuk data validasi\n",
    "y_valid_decoded = le.inverse_transform(y_valid_encoded)\n",
    "\n",
    "# Contoh menampilkan nilai yang telah di-decode\n",
    "print(\"Contoh nilai yang telah di-decode untuk data uji:\")\n",
    "print(y_test_decoded[:10])  # Menampilkan 10 nilai pertama\n",
    "\n",
    "print(\"Contoh nilai yang telah di-decode untuk data validasi:\")\n",
    "print(y_valid_decoded[:10])  # Menampilkan 10 nilai pertama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51b786e-4d11-41bd-a158-20f78d858b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train_bow, y_train_encoded)  # Gunakan X_train_bow atau X_train_tfidf sesuai kebutuhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b881780-0a79-46e0-bba3-a369b732ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred_test = model.predict(X_test_bow)  # Gunakan X_test_bow atau X_test_tfidf sesuai kebutuhan\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_test)\n",
    "report = classification_report(y_test_encoded, y_pred_test)\n",
    "\n",
    "y_pred_valid = model.predict(X_valid_bow)  # Gunakan X_valid_bow atau X_valid_tfidf sesuai kebutuhan\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_pred_valid)\n",
    "report_valid = classification_report(y_valid_encoded, y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeb28e41-0185-4875-9640-4237e11294d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.28      0.37        25\n",
      "           1       0.73      0.75      0.74       147\n",
      "           2       0.81      0.79      0.80       594\n",
      "           3       0.42      0.29      0.34        68\n",
      "           4       0.48      0.72      0.58       123\n",
      "           5       0.66      0.62      0.64       250\n",
      "\n",
      "    accuracy                           0.71      1207\n",
      "   macro avg       0.61      0.58      0.58      1207\n",
      "weighted avg       0.71      0.71      0.70      1207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b7def5-defe-46d8-a82c-c92c41544fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.19      0.25        16\n",
      "           1       0.81      0.81      0.81        98\n",
      "           2       0.81      0.81      0.81       396\n",
      "           3       0.48      0.28      0.36        46\n",
      "           4       0.47      0.70      0.56        83\n",
      "           5       0.71      0.66      0.69       167\n",
      "\n",
      "    accuracy                           0.72       806\n",
      "   macro avg       0.61      0.57      0.58       806\n",
      "weighted avg       0.73      0.72      0.72       806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c166988b-6b0c-46fe-a2fb-4006622fac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train_tfidf, y_train_encoded)  # Gunakan X_train_bow atau X_train_tfidf sesuai kebutuhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c3a769-dcc1-4bbb-adf0-4b4806e7c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred_test = model.predict(X_test_tfidf)  # Gunakan X_test_bow atau X_test_tfidf sesuai kebutuhan\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_test)\n",
    "report = classification_report(y_test_encoded, y_pred_test)\n",
    "\n",
    "y_pred_valid = model.predict(X_valid_tfidf)  # Gunakan X_valid_bow atau X_valid_tfidf sesuai kebutuhan\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_pred_valid)\n",
    "report_valid = classification_report(y_valid_encoded, y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "860a3d8a-fccc-4ef1-a591-5d8111fca757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.28      0.33        25\n",
      "           1       0.70      0.71      0.71       147\n",
      "           2       0.80      0.74      0.77       594\n",
      "           3       0.38      0.43      0.40        68\n",
      "           4       0.45      0.65      0.53       123\n",
      "           5       0.63      0.60      0.62       250\n",
      "\n",
      "    accuracy                           0.67      1207\n",
      "   macro avg       0.56      0.57      0.56      1207\n",
      "weighted avg       0.69      0.67      0.68      1207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b751cd57-a972-4f25-ac1a-f6c80ef34b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.19      0.24        16\n",
      "           1       0.77      0.77      0.77        98\n",
      "           2       0.81      0.77      0.79       396\n",
      "           3       0.33      0.24      0.28        46\n",
      "           4       0.42      0.66      0.52        83\n",
      "           5       0.66      0.62      0.64       167\n",
      "\n",
      "    accuracy                           0.69       806\n",
      "   macro avg       0.56      0.54      0.54       806\n",
      "weighted avg       0.70      0.69      0.69       806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7100788-0a9e-4b17-b9fb-cac98f16887d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Melakukan hyperparameter tuning pada data latih\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Mendapatkan parameter terbaik\u001b[39;00m\n\u001b[0;32m     23\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definisi model Decision Tree\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Daftar hyperparameter yang akan diuji\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30],  \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4],  \n",
    "}\n",
    "\n",
    "# Inisialisasi GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Melakukan hyperparameter tuning pada data latih\n",
    "grid_search.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Mendapatkan parameter terbaik\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Membuat model dengan parameter terbaik\n",
    "best_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "# Melatih model dengan parameter terbaik pada data latih\n",
    "best_model.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Prediksi data uji dan validasi\n",
    "y_pred_test = best_model.predict(X_test_tfidf)\n",
    "y_pred_valid = best_model.predict(X_valid_tfidf)\n",
    "\n",
    "# Menghitung akurasi dan laporan klasifikasi\n",
    "accuracy_test = accuracy_score(y_test_encoded, y_pred_test)\n",
    "report_test = classification_report(y_test_encoded, y_pred_test)\n",
    "\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_pred_valid)\n",
    "report_valid = classification_report(y_valid_encoded, y_pred_valid)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test Report:\")\n",
    "print(report_test)\n",
    "print(\"Validation Report:\")\n",
    "print(report_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef04dcf-0849-4a9a-b377-0a23980cf2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definisi model Logistic Regression\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Daftar hyperparameter yang akan diuji\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # C merupakan kebalikan dari kekuatan regularisasi\n",
    "    'penalty': ['l1', 'l2'],  # Jenis regularisasi\n",
    "}\n",
    "\n",
    "# Inisialisasi GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Melakukan hyperparameter tuning pada data latih\n",
    "grid_search.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Mendapatkan parameter terbaik\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Membuat model dengan parameter terbaik\n",
    "best_model = LogisticRegression(**best_params)\n",
    "\n",
    "# Melatih model dengan parameter terbaik pada data latih\n",
    "best_model.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Prediksi data uji dan validasi\n",
    "y_pred_test = best_model.predict(X_test_tfidf)\n",
    "y_pred_valid = best_model.predict(X_valid_tfidf)\n",
    "\n",
    "# Menghitung akurasi dan laporan klasifikasi\n",
    "accuracy_test = accuracy_score(y_test_encoded, y_pred_test)\n",
    "report_test = classification_report(y_test_encoded, y_pred_test)\n",
    "\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_pred_valid)\n",
    "report_valid = classification_report(y_valid_encoded, y_pred_valid)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test Report:\")\n",
    "print(report_test)\n",
    "print(\"Validation Report:\")\n",
    "print(report_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d379c3-9ad9-4898-83c0-579a859a3ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1489c9b-8fc0-46c1-b794-0ebaff32a1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe91a53-00f8-4b1e-b090-1009d2c64fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5995c-ca82-4a78-a0a5-8232288d9223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed3e1f-dcf9-4601-ba24-f519e4061acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9042b31-82ed-41a7-be97-2b4f9c377d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Inisialisasi Count Vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Melakukan fit transform pada data 'title' menggunakan Count Vectorizer\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Mendapatkan daftar fitur (kata-kata) dari Count Vectorizer\n",
    "train_text_features = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Normalisasi hasil Count Vectorizer\n",
    "X_train_bow_normalized = normalize(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ce54d3-485c-4f12-82db-788d6bc0b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['title'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d46ba97-aa6b-429d-9ae4-cd7886ef5267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0cd7fe2-4559-4ad4-9763-8f2b1e05bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix After One Hot Encoding\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Cetak shape dari matriks setelah one-hot encoding\n",
    "print(\"Shape of Matrix After One Hot Encoding\")\n",
    "print(X_train_bow_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d43d20-8788-4a59-a052-1007c6f69b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF Matrix:\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Menginisialisasi objek TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Anda dapat mengatur jumlah maksimum fitur sesuai kebutuhan\n",
    "\n",
    "# Melakukan transformasi TF-IDF pada kolom 'title' di X_train\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Sekarang X_train_tfidf berisi representasi TF-IDF dari teks di kolom 'title'\n",
    "\n",
    "# Anda dapat mencetak bentuk matriks TF-IDF\n",
    "print(\"Shape of TF-IDF Matrix:\")\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55155ef-5761-4e6e-833b-7ee40f7bb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stopwords from the NLTK library\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(df, column_name):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x).lower())\n",
    "    \n",
    "    # Remove stopwords and join the words with a single space\n",
    "    df[column_name] = df[column_name].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Membersihkan teks dari karakter khusus dan mengonversi teks menjadi huruf kecil\n",
    "X_train = preprocess_text(X_train, \"title\")\n",
    "X_test = preprocess_text(X_test, \"title\")\n",
    "X_valid = preprocess_text(X_valid, \"title\")\n",
    "\n",
    "lalu untuk data target atau y_train, y_test, dan y_valid\n",
    "\n",
    "Nah, tolong bantu saya untuk melanjutkan One Hot Encoding of Text Data (BoW) dan TFIDF of Text Data untuk data X_train, X_test, dan X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ddf82a-d7e7-4129-9bd4-051b5d8ed82e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c628124-6921-4253-8756-8b09db2b9c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
