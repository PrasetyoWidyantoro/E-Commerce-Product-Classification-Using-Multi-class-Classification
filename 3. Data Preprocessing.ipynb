{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c1a1f1-bb3d-44d1-9e0d-fb34696dcd5c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c195396-0a06-4cef-9275-3e8cac5560b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import library untuk data preparation dan visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import pickle and json file for columns and model file\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import yaml\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import src.util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08c84ad7-414b-4fa0-93d5-e06dd86193fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dir = \"config/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f52ae25-08ea-4496-b91a-801516a12b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(param_dir):\n",
    "    with open(param_dir, 'r') as file:\n",
    "        params = yaml.safe_load(file)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6788083-8256-4fd0-8c23-7d35e4260018",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_params(params_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aec8e7f4-73d4-48db-8be0-5b1209bde71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_dataset_dir': 'dataset/1 - raw data/',\n",
       " 'train_set_path': ['dataset/2 - processed/X_train.pkl',\n",
       "  'dataset/2 - processed/y_train.pkl'],\n",
       " 'valid_set_path': ['dataset/2 - processed/X_valid.pkl',\n",
       "  'dataset/2 - processed/y_valid.pkl'],\n",
       " 'test_set_path': ['dataset/2 - processed/X_test.pkl',\n",
       "  'dataset/2 - processed/y_test.pkl'],\n",
       " 'train_bow_set_path': ['dataset/3 - final/X_train_bow.pkl',\n",
       "  'dataset/3 - final/y_train_encoded.pkl'],\n",
       " 'valid_bow_set_path': ['dataset/3 - final/X_valid_bow.pkl',\n",
       "  'dataset/3 - final/y_valid_encoded.pkl'],\n",
       " 'test_bow_set_path': ['dataset/3 - final/X_test_bow.pkl',\n",
       "  'dataset/3 - final/y_test_encoded.pkl'],\n",
       " 'train_tfidf_set_path': ['dataset/3 - final/X_train_tfidf.pkl',\n",
       "  'dataset/3 - final/y_train_encoded.pkl'],\n",
       " 'valid_tfidf_set_path': ['dataset/3 - final/X_valid_tfidf.pkl',\n",
       "  'dataset/3 - final/y_valid_encoded.pkl'],\n",
       " 'test_tfidf_set_path': ['dataset/3 - final/X_test_feng.pkl',\n",
       "  'dataset/3 - final/X_test_tfidf.pkl'],\n",
       " 'model_wordwvec': 'model/word2vec.model',\n",
       " 'count_vectorizer': 'model/count_vectorizer_model.pkl',\n",
       " 'tfidf_vectorizer': 'model/tfidf_vectorizer_model.pkl',\n",
       " 'label_encoder': 'model/label_encoder.pkl',\n",
       " 'model_final': 'model/lr_best_cv.pkl',\n",
       " 'print_debug': True}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9761601-d1d8-4088-93c2-f204a0ce2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = util.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c99a8a-a015-4658-b28b-636fa3faec3d",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5d03175-385d-44c2-94aa-027778a11ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = util.pickle_load(config_data[\"train_set_path\"][0])\n",
    "y_train = util.pickle_load(config_data[\"train_set_path\"][1])\n",
    "\n",
    "X_valid = util.pickle_load(config_data[\"valid_set_path\"][0])\n",
    "y_valid = util.pickle_load(config_data[\"valid_set_path\"][1])\n",
    "\n",
    "X_test = util.pickle_load(config_data[\"test_set_path\"][0])\n",
    "y_test = util.pickle_load(config_data[\"test_set_path\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f784e85-4e16-434b-ba00-39e3cfeec410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>Imax 84532 Tenor Large Recycled Glass Vase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120</th>\n",
       "      <td>Darice 6202-113 10-Light 5-Point Star Tree Top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>5 Piece Full Size Frozen Bedding Set Includes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>New Star Foodservice Knives (Set of 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13665</th>\n",
       "      <td>Mr &amp;amp; Mrs T Bold &amp;amp; Spicy Bloody Mary Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19690</th>\n",
       "      <td>Aviditi MD24126 Multi-Depth Corrugated Box, 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15580</th>\n",
       "      <td>Perfume Studio&amp;reg; Graduated Glass Dropper Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10534</th>\n",
       "      <td>Brother HL-L2340DW Compact Laser Printer, Mono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>Hello Kitty Wastebasket - Garbage Can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7250</th>\n",
       "      <td>Newport Storage Platform Bed Finish: Tan, Size...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8052 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title\n",
       "5544          Imax 84532 Tenor Large Recycled Glass Vase\n",
       "3120   Darice 6202-113 10-Light 5-Point Star Tree Top...\n",
       "1387   5 Piece Full Size Frozen Bedding Set Includes ...\n",
       "1075             New Star Foodservice Knives (Set of 12)\n",
       "13665  Mr &amp; Mrs T Bold &amp; Spicy Bloody Mary Mi...\n",
       "...                                                  ...\n",
       "19690  Aviditi MD24126 Multi-Depth Corrugated Box, 24...\n",
       "15580  Perfume Studio&reg; Graduated Glass Dropper Bo...\n",
       "10534  Brother HL-L2340DW Compact Laser Printer, Mono...\n",
       "2793               Hello Kitty Wastebasket - Garbage Can\n",
       "7250   Newport Storage Platform Bed Finish: Tan, Size...\n",
       "\n",
       "[8052 rows x 1 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f81a3f27-3540-42c2-b407-88fae165b016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544              Home & Kitchen\n",
       "3120              Home & Kitchen\n",
       "1387              Home & Kitchen\n",
       "1075              Home & Kitchen\n",
       "13665     Grocery & Gourmet Food\n",
       "                  ...           \n",
       "19690            Office Products\n",
       "15580    Industrial & Scientific\n",
       "10534            Office Products\n",
       "2793              Home & Kitchen\n",
       "7250              Home & Kitchen\n",
       "Name: category, Length: 8052, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44ecb653-932c-47d6-b67e-dd49905edef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stopwords from the NLTK library\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(df, column_name):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x).lower())\n",
    "    \n",
    "    # Remove stopwords and join the words with a single space\n",
    "    df[column_name] = df[column_name].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb2500d4-1ecb-4f12-858d-3115b6860127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan teks dari karakter khusus dan mengonversi teks menjadi huruf kecil\n",
    "X_train = preprocess_text(X_train, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87b6901d-750a-4ec1-b864-8a82b3655565",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess_text(X_test, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dcbd1284-d13d-4fa4-a200-c9e8b6650e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = preprocess_text(X_valid, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0dec50b-eeea-483c-9b69-51c0ee50e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "\n",
    "# Inisialisasi CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fitting hanya pada data pelatihan\n",
    "count_vectorizer.fit(X_train['title'])\n",
    "\n",
    "with open(config_data[\"count_vectorizer\"], 'wb') as f:\n",
    "    pickle.dump(count_vectorizer, f)\n",
    "    \n",
    "# count_vectorizer = joblib.load(config_data[\"count_vectorizer\"])\n",
    "\n",
    "# Transformasi pada data pelatihan, data uji, dan data validasi\n",
    "X_train_bow = count_vectorizer.transform(X_train['title'])\n",
    "X_test_bow = count_vectorizer.transform(X_test['title'])\n",
    "X_valid_bow = count_vectorizer.transform(X_valid['title'])\n",
    "\n",
    "\n",
    "# Kemudian, di kemudian hari, Anda dapat memuat model CountVectorizer dan menggunakannya pada data baru\n",
    "# untuk melakukan transformasi seperti ini:\n",
    "\n",
    "# Memuat model CountVectorizer\n",
    "# loaded_count_vectorizer = joblib.load('count_vectorizer_model.pkl')\n",
    "\n",
    "# Menggunakan model CountVectorizer untuk melakukan transformasi pada data baru\n",
    "# new_data = [\"Contoh teks baru untuk di-encode\"]\n",
    "# X_new_bow = loaded_count_vectorizer.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0f585be-1e3c-47c1-ba36-6890fb43cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# Inisialisasi TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fitting hanya pada data pelatihan\n",
    "tfidf_vectorizer.fit(X_train['title'])\n",
    "\n",
    "# Menyimpan model TfidfVectorizer ke dalam file\n",
    "with open(config_data[\"tfidf_vectorizer\"], 'wb') as f:\n",
    "    pickle.dump(count_vectorizer, f)\n",
    "    \n",
    "# tfidf_vectorizer = joblib.load(config_data[\"tfidf_vectorizer\"])\n",
    "\n",
    "# Transformasi pada data pelatihan, data uji, dan data validasi\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train['title'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid['title'])\n",
    "\n",
    "\n",
    "    \n",
    "# Kemudian, di kemudian hari, Anda dapat memuat model TfidfVectorizer dan menggunakannya pada data baru\n",
    "# untuk melakukan transformasi seperti ini:\n",
    "# Memuat model TfidfVectorizer\n",
    "# loaded_tfidf_vectorizer = joblib.load('tfidf_vectorizer_model.pkl')\n",
    "# X_new_tfidf = loaded_tfidf_vectorizer.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ae49c1f-4625-4871-9dc4-88fe70afe260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\n\\n# One-Hot Encoding (BoW) pada kolom 'title'\\ncount_vectorizer = CountVectorizer()\\nX_train_bow = count_vectorizer.fit_transform(X_train['title'])\\nX_test_bow = count_vectorizer.transform(X_test['title'])\\nX_valid_bow = count_vectorizer.transform(X_valid['title'])\\n\\n# TF-IDF pada kolom 'title'\\ntfidf_vectorizer = TfidfVectorizer()\\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train['title'])\\nX_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\\nX_valid_tfidf = tfidf_vectorizer.transform(X_valid['title'])\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# One-Hot Encoding (BoW) pada kolom 'title'\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train['title'])\n",
    "X_test_bow = count_vectorizer.transform(X_test['title'])\n",
    "X_valid_bow = count_vectorizer.transform(X_valid['title'])\n",
    "\n",
    "# TF-IDF pada kolom 'title'\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['title'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid['title'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "533acafd-9a6d-443c-a707-34a2c2f3667c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8052x20797 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 79263 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fe5feec-c3cf-41ff-b965-4310de4e4feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8052x20797 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 79263 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "975c4259-f6b9-405c-aa4d-58e7aa52eaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544              Home & Kitchen\n",
       "3120              Home & Kitchen\n",
       "1387              Home & Kitchen\n",
       "1075              Home & Kitchen\n",
       "13665     Grocery & Gourmet Food\n",
       "                  ...           \n",
       "19690            Office Products\n",
       "15580    Industrial & Scientific\n",
       "10534            Office Products\n",
       "2793              Home & Kitchen\n",
       "7250              Home & Kitchen\n",
       "Name: category, Length: 8052, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cae5763-b9b1-4dab-87e3-26fe43f0ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "y_valid_encoded = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2956147f-e5ee-4ddf-ba33-cabbf4a90449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kategori yang diubah ke bilangan bulat:\n",
      "Electronics -> 0\n",
      "Grocery & Gourmet Food -> 1\n",
      "Home & Kitchen -> 2\n",
      "Industrial & Scientific -> 3\n",
      "Office Products -> 4\n",
      "Tools & Home Improvement -> 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Inisialisasi LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit dan transform label\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "# Daftar kategori yang telah diubah ke bilangan bulat\n",
    "encoded_classes = list(le.classes_)\n",
    "\n",
    "# Mencetak hasil\n",
    "print(\"Kategori yang diubah ke bilangan bulat:\")\n",
    "for kategori, kode in zip(encoded_classes, le.transform(encoded_classes)):\n",
    "    print(f\"{kategori} -> {kode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38f5bc-f48d-4534-9da1-397d096e8731",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54cc5aa1-605e-4608-a597-80807ff8313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pickle_dump(X_train_bow, config_data[\"train_bow_set_path\"][0])\n",
    "util.pickle_dump(y_train_encoded, config_data[\"train_bow_set_path\"][1])\n",
    "    \n",
    "util.pickle_dump(X_valid_bow, config_data[\"valid_bow_set_path\"][0])\n",
    "util.pickle_dump(y_valid_encoded, config_data[\"valid_bow_set_path\"][1])\n",
    "\n",
    "util.pickle_dump(X_test_bow, config_data[\"test_bow_set_path\"][0])\n",
    "util.pickle_dump(y_test_encoded, config_data[\"test_bow_set_path\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd770fab-acb0-49ce-8b8e-5c69cca38d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pickle_dump(X_train_tfidf, config_data[\"train_tfidf_set_path\"][0])\n",
    "util.pickle_dump(y_train_encoded, config_data[\"train_tfidf_set_path\"][1])\n",
    "    \n",
    "util.pickle_dump(X_valid_tfidf, config_data[\"valid_tfidf_set_path\"][0])\n",
    "util.pickle_dump(y_valid_encoded, config_data[\"valid_tfidf_set_path\"][1])\n",
    "\n",
    "util.pickle_dump(X_test_tfidf, config_data[\"test_tfidf_set_path\"][0])\n",
    "util.pickle_dump(y_test_encoded, config_data[\"test_tfidf_set_path\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a99680-f07b-46fa-8553-fd239eb8cfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd4fa9-63cb-46cf-95cb-2be0875cca69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50cddcf-7ab0-4d2e-930c-6d1f6244f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi dan fit TfidfVectorizer\n",
    "def fit_tfidf_vectorizer(X_train):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit(X_train['title'])\n",
    "    \n",
    "    # Simpan TfidfVectorizer\n",
    "    with open(config_data[\"tfidf_vectorizer\"], 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "    return tfidf_vectorizer\n",
    "\n",
    "# Load TfidfVectorizer dari file\n",
    "def load_tfidf_vectorizer(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        tfidf_vectorizer = pickle.load(f)\n",
    "    return tfidf_vectorizer\n",
    "\n",
    "\n",
    "# Inisialisasi dan latih LabelEncoder\n",
    "def fit_label_encoder(y_train, save_path):\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(y_train)\n",
    "    \n",
    "    # Simpan LabelEncoder ke dalam file\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    return label_encoder\n",
    "\n",
    "# Load LabelEncoder dari file\n",
    "def load_label_encoder(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    return label_encoder\n",
    "\n",
    "# Inisialisasi dan latih LabelEncoder\n",
    "label_encoder = fit_label_encoder(y_train, config_data[\"label_encoder\"])\n",
    "\n",
    "# Melakukan label encoding pada data pelatihan\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "# Melakukan label encoding pada data uji\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "# Melakukan label encoding pada data validasi\n",
    "y_valid_encoded = label_encoder.transform(y_valid)\n",
    "\n",
    "# Inisialisasi dan latih TfidfVectorizer\n",
    "tfidf_vectorizer = fit_tfidf_vectorizer(X_train, config_data[\"tfidf_vectorizer\"])\n",
    "\n",
    "# Transformasi data pelatihan\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train['title'])\n",
    "# Transformasi data uji\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\n",
    "# Transformasi data validasi\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid['title'])\n",
    "\n",
    "\n",
    "# Memuat TfidfVectorizer dari file\n",
    "tfidf_vectorizer = load_tfidf_vectorizer(config_data[\"tfidf_vectorizer\"])\n",
    "\n",
    "# Transformasi data pelatihan\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train['title'])\n",
    "# Transformasi data uji\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\n",
    "# Transformasi data validasi\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
